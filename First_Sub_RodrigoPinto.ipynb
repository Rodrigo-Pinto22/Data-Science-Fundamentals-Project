{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h1>Correlation between Oil and Fuel Prices in Portugal</h1>\n",
    "<h2>FCD 1st intermediate project presentation</h2>\n",
    "<h3>Rodrigo Pinto 103280 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oil is one of the most important and widely used commodities in the world, since it is the primary global energy source. Derived from fossilized organic matter that accumulated in geological layers over millions of years, crude oil is refined into products such as gasoline, diesel, kerosene, fuel oil, and other petrochemical derivatives, which are essential for various industries, transportation, and energy generation.\n",
    "\n",
    "The evolution of oil prices has been marked by highs and lows, with several economic, political, and environmental crises playing an important role.\n",
    "\n",
    "The rise in oil prices directly affects the prices of fuels such as gasoline and diesel. However, it is important to understand that taxation also has an impact on the final price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>State of the art</h2>\n",
    "</div>\n",
    "\n",
    "In every project, is mandatory doing an initial research. Therefore, I found a brief description about the work of the 2nd place of last year Arquivo.pt awards, which was Habitação.NET.\n",
    "\n",
    "The main goal of this project is to allow the user to explore the evolution of the housing and rental market in Portugal. Besides, the author of this project used as source of information of about 30 million adds from different real estate web pages and 15 thousand news stored in arquivo.pt since 2001. Additionally, Habitação.NET offers the possibility to analyse the price trends at a national level as well as between districts, municipalities and parishes.\n",
    "\n",
    "The data retrieving and processing methodology that the author employed in his project is the following:\n",
    "\n",
    "- ##### Listing real estate web browsers \n",
    "    Use of manual search in Arquivo.pt with API Full-text Search\n",
    "\t<p>Research on Marktest platform</p>\n",
    "<br></br>\n",
    "- ##### Identify the adds\n",
    "    Mapping the structure of each platform's pages\n",
    "\t<p>Finding the relevants adds</p>\n",
    "\tProcessed pages : 32 million by using API CDX of Arquivo.pt\n",
    "<br></br>\n",
    "- ##### Retrieve and Analyse the data\n",
    "    Download the proccessed pages by using API Wayback from arquivo.pt\n",
    "\t<p>FYI it was about 296GB of data</p>\n",
    "\tRE parameter extracte: Price, Area, Type, Location and description\n",
    "<br></br>\n",
    "- ##### Unify the data\n",
    "    Each RE platform uses different nomenclature for the same type of information, so there was a need to normalize it by tuning the \tdata into a unified format.\n",
    "<br></br>\n",
    "- ##### Extract the location of each real estate\n",
    "    There was a lack of consistency about the actual place of each piece of real estate betwween plataforms. So the author used the maps published by the Direção Geral do Território and the OpenStreetMap Nominatim API for the mapping of locations.\n",
    "<br></br>\n",
    "- ##### Filter the data\n",
    "    Commercial and industrial places were discarded from the data set.\n",
    "<br></br>\n",
    "- ##### Calculate the average price of the m2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2>Methodology</h2>\n",
    "</div>\n",
    "\n",
    "As mentioned before, the main goal is to do a correlation between the oil prices and fuel prices in Portugal. In order to accomplish that, it is crucial to gather the right information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Initial Approach</h3>\n",
    "</div>\n",
    "\n",
    "Initially, I analysed all the web pages stored in the arquivo.pt related to fuel prices and I came up with 2 predominants web pages (compareomercado, maisgasolina). Although I didn't find any web page related to oil prices, I seacrhed outside arquivo.pt and I discovered this web page https://www.kaggle.com/datasets/mabusalah/brent-oil-prices that allows the user to download a csv file with a data set that was retrieved from the U.S Energy Information Administration: Europe Brent Spot Price (Dollars per Barrel) that contains the oil price since 1987."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(dates, link_url):\n",
    "    html = \"\"         \n",
    "    \n",
    "    \n",
    "    #extract dates\n",
    "    datas = dates['tstamp']\n",
    "    y = datas[0:4]\n",
    "    m = datas[4:6]\n",
    "    d = datas[6:8]\n",
    "    h = datas[8:10]\n",
    "    min = datas[10:12]\n",
    "    s = datas[12:]\n",
    "\n",
    "    final_date = y+\"-\"+m+\"-\"+d\n",
    "    print(f\"{d}/{m}/{y} at {h}:{min}:{s}\")\n",
    "\n",
    "    #extract the link to the text\n",
    "    original_url = dates['linkToExtractedText']\n",
    "    link_url.append(original_url)\n",
    "\n",
    "\n",
    "    return link_url, final_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: maisgasolina.com\n",
      "\n",
      "Success!\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/09/2009 at 08:53:35\n",
      "FAIL\n",
      "PASS\n",
      "04/08/2010 at 10:40:17\n",
      "PASS\n",
      "22/01/2012 at 11:36:57\n",
      "PASS\n",
      "05/11/2013 at 21:53:54\n",
      "PASS\n",
      "21/01/2011 at 21:53:26\n",
      "PASS\n",
      "22/05/2011 at 00:35:01\n",
      "PASS\n",
      "19/12/2009 at 14:59:57\n",
      "PASS\n",
      "29/05/2010 at 06:22:41\n",
      "PASS\n",
      "24/06/2009 at 00:04:18\n",
      "PASS\n",
      "02/07/2011 at 08:59:15\n",
      "PASS\n",
      "18/02/2008 at 20:37:52\n",
      "PASS\n",
      "22/10/2008 at 11:25:25\n",
      "FAIL\n",
      "PASS\n",
      "22/01/2012 at 11:54:42\n",
      "PASS\n",
      "04/08/2010 at 10:46:08\n",
      "PASS\n",
      "30/09/2009 at 09:00:26\n",
      "PASS\n",
      "24/06/2009 at 00:17:09\n",
      "PASS\n",
      "01/04/2019 at 11:02:06\n",
      "PASS\n",
      "22/05/2011 at 00:41:58\n",
      "PASS\n",
      "21/01/2011 at 22:00:59\n",
      "PASS\n",
      "02/07/2011 at 09:06:23\n",
      "PASS\n",
      "12/07/2018 at 06:33:46\n",
      "PASS\n",
      "29/05/2010 at 06:27:41\n",
      "FAIL\n",
      "PASS\n",
      "19/12/2009 at 15:05:43\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "27/06/2017 at 20:44:18\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "05/11/2013 at 21:59:03\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "26/09/2009 at 17:36:30\n",
      "PASS\n",
      "10/01/2016 at 17:55:33\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "22/05/2019 at 15:01:24\n",
      "PASS\n",
      "26/06/2019 at 21:33:18\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "03/10/2019 at 14:02:12\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "22/12/2019 at 13:05:00\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/03/2018 at 18:39:03\n",
      "PASS\n",
      "31/03/2018 at 17:47:17\n",
      "PASS\n",
      "20/06/2016 at 23:47:47\n",
      "FAIL\n",
      "PASS\n",
      "02/06/2020 at 04:12:55\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "09/11/2019 at 18:15:48\n",
      "PASS\n",
      "08/10/2019 at 17:17:39\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/03/2020 at 08:46:16\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "28/02/2020 at 18:08:29\n",
      "PASS\n",
      "08/03/2020 at 18:17:23\n",
      "PASS\n",
      "19/11/2019 at 00:33:23\n",
      "PASS\n",
      "02/06/2020 at 04:14:15\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/09/2019 at 07:31:15\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "02/04/2020 at 17:07:01\n",
      "PASS\n",
      "03/04/2020 at 17:07:15\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "22/12/2019 at 13:06:02\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "01/09/2020 at 17:42:08\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "17/07/2018 at 14:11:09\n",
      "PASS\n",
      "28/07/2018 at 17:41:29\n",
      "FAIL\n",
      "PASS\n",
      "30/10/2018 at 18:22:19\n",
      "PASS\n",
      "20/10/2018 at 17:21:54\n",
      "PASS\n",
      "12/07/2019 at 18:03:41\n",
      "PASS\n",
      "23/07/2019 at 19:26:42\n",
      "PASS\n",
      "30/03/2020 at 08:47:57\n",
      "FAIL\n",
      "PASS\n",
      "09/01/2019 at 19:05:01\n",
      "PASS\n",
      "08/02/2019 at 19:03:42\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "01/09/2020 at 17:43:50\n",
      "FAIL\n",
      "PASS\n",
      "05/04/2019 at 18:26:56\n",
      "PASS\n",
      "16/04/2019 at 18:09:47\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "13/11/2015 at 05:59:23\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "21/06/2016 at 07:22:09\n",
      "FAIL\n",
      "PASS\n",
      "19/11/2019 at 00:37:35\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "13/11/2015 at 06:06:17\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "10/02/2017 at 09:16:35\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "09/05/2018 at 18:14:09\n",
      "PASS\n",
      "08/06/2018 at 17:39:31\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "12/11/2012 at 16:03:03\n",
      "PASS\n",
      "26/10/2012 at 15:02:55\n",
      "PASS\n",
      "14/10/2018 at 23:32:17\n",
      "PASS\n",
      "12/01/2013 at 16:03:12\n",
      "PASS\n",
      "17/01/2013 at 16:03:07\n",
      "PASS\n",
      "01/08/2017 at 05:48:52\n",
      "FAIL\n",
      "PASS\n",
      "29/08/2013 at 16:02:54\n",
      "PASS\n",
      "03/09/2013 at 16:02:53\n",
      "PASS\n",
      "05/10/2013 at 16:02:54\n",
      "PASS\n",
      "04/10/2013 at 16:02:55\n",
      "PASS\n",
      "08/12/2017 at 13:15:00\n",
      "FAIL\n",
      "PASS\n",
      "08/04/2018 at 10:40:04\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "14/03/2019 at 20:12:50\n",
      "FAIL\n",
      "PASS\n",
      "14/10/2018 at 23:40:21\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "01/08/2017 at 06:02:59\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "19/03/2019 at 19:19:59\n",
      "FAIL\n",
      "PASS\n",
      "27/09/2017 at 12:35:50\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "08/12/2017 at 13:31:04\n",
      "FAIL\n",
      "PASS\n",
      "01/11/2016 at 04:41:23\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "08/04/2018 at 10:56:30\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "01/11/2016 at 13:05:20\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "23/12/2009 at 03:08:35\n",
      "FAIL\n",
      "PASS\n",
      "12/07/2018 at 06:44:25\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "23/12/2009 at 02:53:04\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "14/03/2019 at 19:17:33\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "14/06/2016 at 09:24:40\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "04/12/2015 at 14:50:08\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "27/09/2019 at 23:30:23\n",
      "PASS\n",
      "26/03/2020 at 06:07:31\n",
      "PASS\n",
      "01/01/2020 at 04:30:28\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "26/06/2020 at 18:43:39\n",
      "FAIL\n",
      "PASS\n",
      "15/08/2017 at 17:35:59\n",
      "PASS\n",
      "21/12/2017 at 17:57:55\n",
      "PASS\n",
      "24/04/2018 at 07:04:06\n",
      "PASS\n",
      "30/09/2014 at 18:05:29\n",
      "PASS\n",
      "28/02/2017 at 21:57:13\n",
      "PASS\n",
      "13/11/2016 at 19:44:45\n",
      "FAIL\n",
      "PASS\n",
      "16/11/2013 at 18:19:29\n",
      "PASS\n",
      "22/09/2020 at 14:50:24\n",
      "FAIL\n",
      "PASS\n",
      "07/11/2013 at 20:43:01\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "02/06/2020 at 10:47:34\n",
      "PASS\n",
      "14/10/2019 at 14:56:48\n",
      "PASS\n",
      "09/01/2020 at 23:45:08\n",
      "PASS\n",
      "01/09/2020 at 23:38:50\n",
      "FAIL\n",
      "PASS\n",
      "06/11/2019 at 03:16:38\n",
      "PASS\n",
      "05/12/2019 at 02:55:36\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "01/02/2020 at 03:01:13\n",
      "PASS\n",
      "26/03/2020 at 03:10:08\n",
      "PASS\n",
      "18/04/2020 at 02:01:06\n",
      "PASS\n",
      "07/05/2020 at 02:11:35\n",
      "PASS\n",
      "09/07/2018 at 15:31:20\n",
      "PASS\n",
      "10/07/2018 at 15:24:15\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/06/2018 at 15:17:17\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "PASS\n",
      "30/06/2018 at 15:44:22\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "FAIL\n",
      "Topic: compareomercado.pt/precos-combustiveis-gasolina-gasoleo\n",
      "\n",
      "Success!\n",
      "PASS\n",
      "06/12/2019 at 04:08:46\n",
      "PASS\n",
      "20/06/2020 at 22:32:46\n",
      "PASS\n",
      "17/11/2019 at 05:00:48\n",
      "PASS\n",
      "14/02/2016 at 05:03:48\n",
      "PASS\n",
      "05/03/2020 at 21:07:58\n",
      "PASS\n",
      "01/09/2019 at 05:37:04\n",
      "PASS\n",
      "09/11/2013 at 04:06:27\n",
      "PASS\n",
      "04/06/2020 at 00:29:43\n",
      "PASS\n",
      "26/04/2015 at 20:16:40\n",
      "PASS\n",
      "02/10/2014 at 10:37:33\n",
      "PASS\n",
      "23/03/2019 at 12:36:03\n",
      "PASS\n",
      "03/09/2020 at 04:17:23\n",
      "PASS\n",
      "15/10/2018 at 05:40:19\n",
      "PASS\n",
      "07/06/2016 at 19:06:41\n",
      "PASS\n",
      "24/02/2017 at 23:12:27\n",
      "PASS\n",
      "26/11/2015 at 03:37:12\n",
      "PASS\n",
      "08/08/2017 at 01:47:50\n",
      "PASS\n",
      "14/12/2017 at 11:30:17\n",
      "PASS\n",
      "16/04/2018 at 13:00:58\n",
      "PASS\n",
      "07/11/2016 at 10:43:43\n",
      "PASS\n",
      "20/07/2018 at 12:41:55\n",
      "FAIL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topics = [\"maisgasolina.com\", \"compareomercado.pt/precos-combustiveis-gasolina-gasoleo\"]\n",
    "items = 500\n",
    "begin =20080218203700\n",
    "last =20230217011100\n",
    "url_link_to_extract = []\n",
    "link_url = []\n",
    "title_list = []\n",
    "prefinal_date_list = []\n",
    "final_date_list = []\n",
    "date_rearrange = []\n",
    "for topic in topics:\n",
    "    print(f\"Topic: {topic}\\n\")\n",
    "\n",
    "    url = f\"https://arquivo.pt/textsearch?q={topic}&maxItems={items}&prettyPrint=true&from={begin}&to={last}\"\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        print(\"Success!\")\n",
    "        data = r.json()\n",
    "        #print(data)\n",
    "        for dates in data['response_items']:\n",
    "            title = dates['title']\n",
    "            if len(title) != 0 and not title.__contains__(\"301 Moved Permanently\") and (title.__contains__('Mais Gasolina') or title.__contains__('Compare o Mercado')):                                                                       \n",
    "                print(\"PASS\")\n",
    "                url_link_to_extract, dates_4_file = extract(dates, link_url)                    \n",
    "                final_date_list.append(dates_4_file)\n",
    "                title_list.append(title)\n",
    "                \n",
    "                #print(new_data)\n",
    "            else:\n",
    "                print(\"FAIL\")\n",
    "    else:\n",
    "        print(\"Something is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sorting the title of each browser link to extracted text by the date \n",
    "ziped = list(zip(final_date_list, url_link_to_extract, title_list))\n",
    "ziped.sort()\n",
    "final_date_list, url_link_to_extract, title_list = zip(*ziped)\n",
    "final_date_list = list(final_date_list)\n",
    "url_link_to_extract = list(url_link_to_extract)\n",
    "title_list = list(title_list)\n",
    "\n",
    "#Creating a dictionary\n",
    "dict_search = {'Title':title_list, 'Link To Extracted Text': url_link_to_extract, 'Date': final_date_list}\n",
    "\n",
    "#Creating a csv file\n",
    "dt_search = pd.DataFrame(data = dict_search)\n",
    "dt_search.to_csv('Web_search', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Mais Gasolina - Preço dos combustíveis em Port...   \n",
      "1  Mais Gasolina - Preço dos combustíveis em Port...   \n",
      "2  Mais Gasolina - Preço dos combustíveis em Port...   \n",
      "3  Postos com o combustível mais barato | Mais Ga...   \n",
      "4  Mais Gasolina - Preços dos combustíveis em Por...   \n",
      "\n",
      "                              Link To Extracted Text        Date  \n",
      "0  https://arquivo.pt/textextracted?m=http%3A%2F%...  2008-02-18  \n",
      "1  https://arquivo.pt/textextracted?m=http%3A%2F%...  2008-10-22  \n",
      "2  https://arquivo.pt/textextracted?m=http%3A%2F%...  2009-06-24  \n",
      "3  https://arquivo.pt/textextracted?m=http%3A%2F%...  2009-06-24  \n",
      "4  https://arquivo.pt/textextracted?m=http%3A%2F%...  2009-09-26  \n"
     ]
    }
   ],
   "source": [
    "print(dt_search.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Current state of the project</h3>\n",
    "</div>\n",
    "\n",
    "At the moment, the project is in the data filtering stage. This stage is crucial as it discards irrelevant information. Therefore, I analyzed the characteristics of the websites' structure over the years to highlight patterns in order to extract information effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "\n",
    "with open(\"C:\\\\Users\\\\ASUS\\\\Documents\\\\GitHub\\\\tutorial\\\\web_search_1\", 'r') as f:\n",
    "    df = pd.read_csv(f, delimiter= ',')\n",
    "\n",
    "\n",
    "list_1,list_2, list_3, list_4, list_5 = [], [], [], [], []\n",
    "\n",
    "list_dates_1,list_dates_2,list_dates_3,list_dates_4,list_dates_5 = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(doc):\n",
    "    return [word for word in doc if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric(f_doc):\n",
    "    str_word = \"\"\n",
    "    list_price = []\n",
    "    price_pattern = r\"\\d{1}[.,]\\d{3}\"  #padrao do preço \\b corresponde à ancora (limite da palavra) \\d{1} um numero decimal [.,] ponto e virgula \\d{3} tres numeros decimais\n",
    "    for word in f_doc:\n",
    "        str_word += word\n",
    "    price = re.findall(price_pattern, str_word)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuel(g, fuel_text):\n",
    "    lista_debug_1, lista_debug_2, lista_debug_2_, lista_debug_2_a, lista_debug_2_aa, lista_debug_3, lista_debug_4, lista_debug_5 =[], [], [], [], [], [], [], []\n",
    "    i_1 =  i_2 =  i_3 = 0\n",
    "    if g == 1:\n",
    "        for word in fuel_text:\n",
    "            i_1+=1\n",
    "            if  word.lower() == \"gasolina\":\n",
    "                word_n= fuel_text[i_1]\n",
    "                lista_debug_1.append(word_n)\n",
    "        return lista_debug_1\n",
    "    elif g ==2 :\n",
    "        for word in fuel_text:\n",
    "            i_2+=1\n",
    "            if  word == \"95\":\n",
    "                word_n= fuel_text[i_2+1]\n",
    "                lista_debug_2.append(word_n)\n",
    "                lista_debug_2_ = numeric(lista_debug_2)\n",
    "                for i in range(len(lista_debug_2_)):\n",
    "                    lista_debug_2_[i] = float(lista_debug_2_[i])\n",
    "                av = round(np.mean(lista_debug_2_),3)\n",
    "                lista_debug_2_.clear()\n",
    "                lista_debug_2_.append(av)\n",
    "                \n",
    "        return  lista_debug_2_\n",
    "    elif g == 3:\n",
    "        for word in fuel_text:\n",
    "            i_3+=1\n",
    "            if  word == \"Repsol\" :#and fuel_text[i_3-1] == \"/\":\n",
    "                word_1= fuel_text[i_3]\n",
    "                word_2 = fuel_text[i_3+5]\n",
    "                word_3 = fuel_text[i_3+10]\n",
    "                i_3 = 0\n",
    "\n",
    "                lista_debug_2_a.append(word_1)\n",
    "                lista_debug_2_a.append(word_2)\n",
    "                lista_debug_2_a.append(word_3)\n",
    "\n",
    "                lista_debug_2_aa = numeric(lista_debug_2_a)\n",
    "                for i in range(len(lista_debug_2_aa)):\n",
    "                    lista_debug_2_aa[i] = float(lista_debug_2_aa[i])\n",
    "                av = round(np.mean(lista_debug_2_aa),3)\n",
    "                lista_debug_2_aa.clear()\n",
    "                lista_debug_2_aa.append(av)\n",
    "        return  lista_debug_2_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "médio\n",
      "do\n",
      "10\n",
      "Mai\n",
      "de\n",
      "Preço\n",
      "2011\n",
      "Preço\n",
      "e\n",
      "descida\n",
      "histórico,\n",
      "e\n",
      "Jan\n",
      "Gasóleo.\n",
      "subir\n",
      "até\n",
      "cent/litro\n",
      "gasóleo\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "em\n",
      "['2008-02-18', '2008-10-22', '2008-02-18', '2008-10-22']\n",
      "lista 1 :['1.374', '1.368', '1.374', '1.359', '1.359', '1.359', '1.374', '1.368', '1.374', '1.359', '1.359', '1.359']\n",
      "\n",
      "lista 2 :['1.569', '1.589', '1.574', '1.569', '1.589', '1.574', '1.584', '1.599', '1.589', '1.624', '1.644', '1.619', '1.664', '1.684', '1.664', '1.654', '1.674', '1.654', '1.649', '1.674', '1.654', '1.634', '1.654', '1.634', '1.619', '1.639', '1.624', '1.644', '1.674', '1.649', '1.619', '1.649', '1.634', '1.463', '1.484', '1.474', '1.489', '1.494', '1.489', '1.524', '1.544', '1.534', '1.539', '1.559', '1.544', '1.594', '1.614', '1.594', '1.629', '1.649', '1.629', '1.659', '1.679', '1.659', '1.564', '1.579', '1.564', '1.589', '1.609', '1.594', '1.594', '1.614', '1.599', '1.584', '1.609', '1.579', '1.564', '1.599', '1.564', '1.569', '1.589', '1.564', '1.564', '1.589', '1.574', '1.569', '1.589', '1.579', '1.604', '1.619', '1.609', '1.559', '1.589', '1.564', '1.419', '1.459', '1.459', '1.334', '1.359', '1.354', '1.334', '1.359', '1.354', '1.449', '1.479', '1.459', '1.489', '1.519', '1.499', '1.569', '1.589', '1.574', '1.569', '1.589', '1.574', '1.584', '1.599', '1.589', '1.624', '1.644', '1.619', '1.664', '1.684', '1.664', '1.654', '1.674', '1.654', '1.649', '1.674', '1.654', '1.634', '1.654', '1.634', '1.619', '1.639', '1.624', '1.644', '1.674', '1.649', '1.619', '1.649', '1.634', '1.463', '1.484', '1.474', '1.489', '1.494', '1.489', '1.524', '1.544', '1.534', '1.539', '1.559', '1.544', '1.594', '1.614', '1.594', '1.629', '1.649', '1.629', '1.659', '1.679', '1.659', '1.564', '1.579', '1.564', '1.589', '1.609', '1.594', '1.594', '1.614', '1.599', '1.584', '1.609', '1.579', '1.564', '1.599', '1.564', '1.569', '1.589', '1.564', '1.564', '1.589', '1.574', '1.569', '1.589', '1.579', '1.604', '1.619', '1.609', '1.559', '1.589', '1.564', '1.419', '1.459', '1.459', '1.334', '1.359', '1.354', '1.334', '1.359', '1.354', '1.449', '1.479', '1.459', '1.489', '1.519', '1.499']\n",
      "\n",
      "198\n",
      "['2009-09-26', '2009-09-30', '2009-12-19', '2010-05-29', '2010-08-04', '2011-01-21', '2011-05-22', '2011-07-02', '2012-01-22', '2012-10-26', '2012-11-12', '2013-01-12', '2013-01-17', '2013-08-29', '2013-09-03', '2013-10-04', '2013-10-05', '2013-11-05', '2018-03-30', '2018-03-31', '2018-04-08', '2018-05-09', '2018-06-08', '2018-07-12', '2018-07-17', '2018-07-28', '2018-10-14', '2018-10-20', '2018-10-30', '2019-01-09', '2019-02-08', '2019-03-14', '2019-04-01', '2019-04-05', '2019-04-16', '2019-05-22', '2019-06-26', '2019-07-12', '2019-07-23', '2019-10-03', '2019-10-08', '2019-11-09', '2019-11-19', '2019-12-22', '2020-02-28', '2020-03-08', '2020-03-30', '2020-04-02', '2020-04-03', '2020-06-02', '2020-09-01', '2009-09-26', '2009-09-30', '2009-12-19', '2010-05-29', '2010-08-04', '2011-01-21', '2011-05-22', '2011-07-02', '2012-01-22', '2012-10-26', '2012-11-12', '2013-01-12', '2013-01-17', '2013-08-29', '2013-09-03', '2013-10-04', '2013-10-05', '2013-11-05', '2018-03-30', '2018-03-31', '2018-04-08', '2018-05-09', '2018-06-08', '2018-07-12', '2018-07-17', '2018-07-28', '2018-10-14', '2018-10-20', '2018-10-30', '2019-01-09', '2019-02-08', '2019-03-14', '2019-04-01', '2019-04-05', '2019-04-16', '2019-05-22', '2019-06-26', '2019-07-12', '2019-07-23', '2019-10-03', '2019-10-08', '2019-11-09', '2019-11-19', '2019-12-22', '2020-02-28', '2020-03-08', '2020-03-30', '2020-04-02', '2020-04-03', '2020-06-02', '2020-09-01']\n",
      "102\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = 0\n",
    "for i in range(0,len(df)):\n",
    "    link = df.iloc[i,1]\n",
    "    date = df.iloc[i,2]\n",
    "    html = urlopen(link).read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    #print(f\"{i}\\n\")\n",
    "    #print()\n",
    "    \n",
    "    str_soup = str(soup)\n",
    "    \n",
    "    if str_soup.__contains__('Mais Gasolina'):\n",
    "        \n",
    "    #print(str_soup.__contains__('Preços de referência'))\n",
    "    #print(\"------------------------------------------------------------------------------------------------\")\n",
    "        #print(soup)\n",
    "        if str_soup[0:13]==\"Mais Gasolina\":\n",
    "            \n",
    "            if str_soup.__contains__('Preços de referência'):\n",
    "                g = 1\n",
    "                str_soup_1 = str_soup.split()               #Passar para lista, retirando os espaço entre palavras\n",
    "                filtered_1 = remove_words(str_soup_1)       #Remover stopwords   \n",
    "                fuel_soup_1 = fuel(g,filtered_1)            #Extrair possiveis preços\n",
    "                num_soup_gasolina_1 = numeric(fuel_soup_1)  #Extrair preços\n",
    "                list_1.extend(num_soup_gasolina_1)          #Guardar preços\n",
    "                list_dates_1.append(date)                   #Guardar respectiva data do ficheiro \n",
    "                \n",
    "\n",
    "            elif str_soup.__contains__(\"Comparador Galp / BP / Repsol\"):\n",
    "                str_soup_2 = str_soup.split()               \n",
    "                a+=1\n",
    "                g = 2            \n",
    "                ###### Definir o índice onde começa e onde termina\n",
    "                start_index = str_soup_2.index('Galp')\n",
    "                end_index = 78+start_index\n",
    "                #print(link)\n",
    "                #print(str_soup_2[end_index])\n",
    "                str_soup_2 = str_soup_2[start_index:end_index]\n",
    "                # Extrair o subgrupo\n",
    "                if str_soup_2[-1].endswith(\"operar\"):       \n",
    "                    filtered_2 = remove_words(str_soup_2)\n",
    "                    #print(filtered_2)\n",
    "                    fuel_soup_2 = fuel(g, filtered_2)\n",
    "                    #print(fuel_soup_2)\n",
    "                    #num_soup_gasolina_2 = numeric(fuel_soup_2)\n",
    "                    list_2.extend(fuel_soup_2)\n",
    "                    list_dates_2.append(date)\n",
    "                    \n",
    "                else:                 \n",
    "                    g = 3\n",
    "                    filtered_2a = remove_words(str_soup_2)\n",
    "                    #print(filtered_2a)\n",
    "                    fuel_soup_2a = fuel(g, filtered_2a)\n",
    "                    #print(fuel_soup_2a)\n",
    "                    #num_soup_gasolina_2a = numeric(fuel_soup_2a)\n",
    "                    list_2.extend(fuel_soup_2a)\n",
    "                    list_dates_2.append(date)\n",
    "                    \n",
    "            else:          \n",
    "                if str_soup.__contains__('Os mais baratos'):\n",
    "                    #print(i)   \n",
    "                    #print(str_soup)\n",
    "                    #print()\n",
    "                    print(\"------------------------------------------3----------------------------------------------------- \\n\")\n",
    "                    print(link)\n",
    "                    str_soup_3 = str_soup.split()\n",
    "                    print(str_soup_3)\n",
    "                    #filtered = remove_words(str_soup)\n",
    "                    #list_3.extend(filtered)\n",
    "    else:\n",
    "        \n",
    "        if str_soup.__contains__('Gasóleo colorido') or str_soup.__contains__('Gasóleo especial') or str_soup.__contains__('Gasóleo simples') or str_soup.__contains__('Gasolina especial 98') or str_soup.__contains__('Gasolina simples 95'):\n",
    "            print(\"------------------------------------------4----------------------------------------------------- \\n\")\n",
    "            print(link)\n",
    "            str_soup_4 = str_soup.split()\n",
    "            print(str_soup_4)\n",
    "            #filtered = remove_words(str_soup)\n",
    "            #list_4.extend(filtered)\n",
    "        else:\n",
    "            print(\"------------------------------------------5----------------------------------------------------- \\n\")\n",
    "            print(link)\n",
    "            str_soup_5 = str_soup.split()\n",
    "            print(str_soup_5)\n",
    "            #filtered = remove_words(str_soup)\n",
    "            #list_5.extend(filtered)\n",
    "\n",
    "\n",
    "\n",
    "print(a)\n",
    "print(list_dates_1)\n",
    "print(f\"lista 1 :{list_1}\\n\")\n",
    "print(f\"lista 2 :{list_2}\\n\")\n",
    "print(len(list_2))\n",
    "print(list_dates_2)\n",
    "print(len(list_dates_2))\n",
    "print(len(list_2)%len(list_dates_2) == 0)\n",
    "#print(f\"lista 2 :{soup}\\n\")\n",
    "#print(f\"lista 3 :{list_3}\\n\")\n",
    "#print(f\"lista 4 :{len(list_4)}\\n\")\n",
    "#print(f\"lista 5 :{len(list_5)}\")\n",
    "\n",
    "#print(gasoleo)\n",
    "#print(gasolina)\n",
    "\n",
    "#age_text = soup.get_text(\" \", strip=True)\n",
    "#list_text = page_text.split()\n",
    "#print(list_text)\n",
    "\n",
    "#pequena alteracao \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(num_soup_gasolina_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Future Steps</h3>\n",
    "</div>\n",
    "\n",
    "- Finish this step of data filtering\n",
    "- Analyse the taxation's impact on fuel's final price\n",
    "- Ploting the retrieved data\n",
    "- Use ML tools to predict the future fuel price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
